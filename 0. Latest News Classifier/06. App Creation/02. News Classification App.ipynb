{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Classification App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "* https://www.w3schools.com/colors/colors_picker.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import punkt\n",
    "from nltk.corpus.reader import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import dash_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performing model is the SVM. We'll use it in the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_models = \"C:/Users/migue/Data Science/Master Data Science/KSCHOOL/9. TFM/0. Latest News Classifier/04. Model Training/Models/\"\n",
    "\n",
    "# SVM\n",
    "path_svm = path_models + 'best_svc.pickle'\n",
    "with open(path_svm, 'rb') as data:\n",
    "    svc_model = pickle.load(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. TF-IDF object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tfidf = \"C:/Users/migue/Data Science/Master Data Science/KSCHOOL/9. TFM/0. Latest News Classifier/03. Feature Engineering/Pickles/tfidf.pickle\"\n",
    "\n",
    "with open(path_tfidf, 'rb') as data:\n",
    "    tfidf = pickle.load(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Category mapping dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_codes = {\n",
    "    'business': 0,\n",
    "    'entertainment': 1,\n",
    "    'politics': 2,\n",
    "    'sport': 3,\n",
    "    'tech': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Definition of functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Web Scraping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El Pais\n",
    "def get_news_elpais():\n",
    "    \n",
    "    # url definition\n",
    "    url = \"https://elpais.com/elpais/inenglish.html\"\n",
    "    \n",
    "    # Request\n",
    "    r1 = requests.get(url)\n",
    "    r1.status_code\n",
    "\n",
    "    # We'll save in coverpage the cover page content\n",
    "    coverpage = r1.content\n",
    "\n",
    "    # Soup creation\n",
    "    soup1 = BeautifulSoup(coverpage, 'html5lib')\n",
    "\n",
    "    # News identification\n",
    "    coverpage_news = soup1.find_all('h2', class_='articulo-titulo')\n",
    "    len(coverpage_news)\n",
    "    \n",
    "    number_of_articles = 5\n",
    "\n",
    "    # Empty lists for content, links and titles\n",
    "    news_contents = []\n",
    "    list_links = []\n",
    "    list_titles = []\n",
    "\n",
    "    for n in np.arange(0, number_of_articles):\n",
    "\n",
    "        # only news articles (there are also albums and other things)\n",
    "        if \"inenglish\" not in coverpage_news[n].find('a')['href']:  \n",
    "            continue\n",
    "\n",
    "        # Getting the link of the article\n",
    "        link = coverpage_news[n].find('a')['href']\n",
    "        list_links.append(link)\n",
    "\n",
    "        # Getting the title\n",
    "        title = coverpage_news[n].find('a').get_text()\n",
    "        list_titles.append(title)\n",
    "\n",
    "        # Reading the content (it is divided in paragraphs)\n",
    "        article = requests.get(link)\n",
    "        article_content = article.content\n",
    "        soup_article = BeautifulSoup(article_content, 'html5lib')\n",
    "        body = soup_article.find_all('div', class_='articulo-cuerpo')\n",
    "        x = body[0].find_all('p')\n",
    "\n",
    "        # Unifying the paragraphs\n",
    "        list_paragraphs = []\n",
    "        for p in np.arange(0, len(x)):\n",
    "            paragraph = x[p].get_text()\n",
    "            list_paragraphs.append(paragraph)\n",
    "            final_article = \" \".join(list_paragraphs)\n",
    "\n",
    "        news_contents.append(final_article)\n",
    "\n",
    "    # df_features\n",
    "    df_features = pd.DataFrame(\n",
    "         {'Content': news_contents \n",
    "        })\n",
    "\n",
    "    # df_show_info\n",
    "    df_show_info = pd.DataFrame(\n",
    "        {'Article Title': list_titles,\n",
    "         'Article Link': list_links})\n",
    "    \n",
    "    return (df_features, df_show_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Feature Engineering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_signs = list(\"?:!.,;\")\n",
    "stop_words = list(stopwords.words('english'))\n",
    "\n",
    "def create_features_from_df(df):\n",
    "    \n",
    "    df['Content_Parsed_1'] = df['Content'].str.replace(\"\\r\", \" \")\n",
    "    df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace(\"\\n\", \" \")\n",
    "    df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace(\"    \", \" \")\n",
    "    df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace('\"', '')\n",
    "    \n",
    "    df['Content_Parsed_2'] = df['Content_Parsed_1'].str.lower()\n",
    "    \n",
    "    df['Content_Parsed_3'] = df['Content_Parsed_2']\n",
    "    for punct_sign in punctuation_signs:\n",
    "        df['Content_Parsed_3'] = df['Content_Parsed_3'].str.replace(punct_sign, '')\n",
    "        \n",
    "    df['Content_Parsed_4'] = df['Content_Parsed_3'].str.replace(\"'s\", \"\")\n",
    "    \n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    nrows = len(df)\n",
    "    lemmatized_text_list = []\n",
    "    for row in range(0, nrows):\n",
    "\n",
    "        # Create an empty list containing lemmatized words\n",
    "        lemmatized_list = []\n",
    "        # Save the text and its words into an object\n",
    "        text = df.loc[row]['Content_Parsed_4']\n",
    "        text_words = text.split(\" \")\n",
    "        # Iterate through every word to lemmatize\n",
    "        for word in text_words:\n",
    "            lemmatized_list.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "        # Join the list\n",
    "        lemmatized_text = \" \".join(lemmatized_list)\n",
    "        # Append to the list containing the texts\n",
    "        lemmatized_text_list.append(lemmatized_text)\n",
    "    \n",
    "    df['Content_Parsed_5'] = lemmatized_text_list\n",
    "    \n",
    "    df['Content_Parsed_6'] = df['Content_Parsed_5']\n",
    "    for stop_word in stop_words:\n",
    "        regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
    "        df['Content_Parsed_6'] = df['Content_Parsed_6'].str.replace(regex_stopword, '')\n",
    "        \n",
    "    df = df['Content_Parsed_6']\n",
    "    df = df.rename(columns={'Content_Parsed_6': 'Content_Parsed'})\n",
    "    \n",
    "    # TF-IDF\n",
    "    features = tfidf.transform(df).toarray()\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_name(category_id):\n",
    "    for category, id_ in category_codes.items():    \n",
    "        if id_ == category_id:\n",
    "            return category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Prediction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_features(features):\n",
    "    \n",
    "    # Predict using the input model\n",
    "    predictions = svc_model.predict(features)\n",
    "    \n",
    "    # Return result\n",
    "    categories = [get_category_name(x) for x in predictions]\n",
    "    \n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_df(df, categories):\n",
    "    df['Prediction'] = categories\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the whole process can be written in these 4 lines of code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Get the scraped dataframes\n",
    "df_features, df_show_info = get_news_elpais()\n",
    "\n",
    "# Create features\n",
    "features = create_features_from_df(df_features)\n",
    "\n",
    "# Predict\n",
    "predictions = predict_from_features(features)\n",
    "\n",
    "# Put into dataset\n",
    "df = complete_df(df_show_info, predictions)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dash App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stylesheet\n",
    "external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
    "\n",
    "app = dash.Dash(__name__, external_stylesheets=external_stylesheets)\n",
    "\n",
    "# Colors\n",
    "colors = {\n",
    "    'background': '#fffaea',\n",
    "    'text': '#696969'\n",
    "}\n",
    "\n",
    "app.layout = html.Div(style={'backgroundColor':colors['background']}, children=[\n",
    "    \n",
    "    # Title\n",
    "    html.H1(children='News Classification App',\n",
    "            style={\n",
    "                'textAlign': 'left',\n",
    "                'color': colors['text']\n",
    "\n",
    "            }),\n",
    "\n",
    "    # Sub-title\n",
    "    html.Div(children='''\n",
    "        Scrape the latest news from different newspapers and show a dashboard.\n",
    "    '''),\n",
    "    \n",
    "    # Checkbox\n",
    "    dcc.Checklist(\n",
    "        options=[\n",
    "            {'label': 'El Pais English', 'value': 'EPE'},\n",
    "            {'label': 'The New York Times', 'value': 'NYT'}\n",
    "        ],\n",
    "        values=['EPE', 'NYT'],\n",
    "        id='checklist'),\n",
    "\n",
    "    # Button\n",
    "    html.Button('Submit', id='submit', type='submit'),\n",
    "    \n",
    "    # Output Block\n",
    "    html.Div(id='output-container-button',\n",
    "             children='Enter a value and press submit')\n",
    "    \n",
    "\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    dash.dependencies.Output('output-container-button', 'children'),\n",
    "    [],\n",
    "    [dash.dependencies.State('checklist', 'values')],\n",
    "    [dash.dependencies.Event('submit', 'click')])\n",
    "\n",
    "def update_output(values):\n",
    "    \n",
    "    if 'EPE' in values:\n",
    "        # Get the scraped dataframes\n",
    "        df_features, df_show_info = get_news_elpais()\n",
    "    \n",
    "    # Create features\n",
    "    features = create_features_from_df(df_features)\n",
    "    # Predict\n",
    "    predictions = predict_from_features(features)\n",
    "    # Put into dataset\n",
    "    df = complete_df(df_show_info, predictions)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:8050/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [29/Dec/2018 19:49:50] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [29/Dec/2018 19:49:51] \"GET /_dash-layout HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [29/Dec/2018 19:49:51] \"GET /_dash-dependencies HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [29/Dec/2018 19:50:00] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [29/Dec/2018 19:50:00] \"GET /_dash-layout HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [29/Dec/2018 19:50:00] \"GET /_dash-dependencies HTTP/1.1\" 200 -\n",
      "[2018-12-29 19:50:08,595] ERROR in app: Exception on /_dash-update-component [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\migue\\Anaconda3\\lib\\site-packages\\dash\\dash.py\", line 918, in add_context\n",
      "    cls=plotly.utils.PlotlyJSONEncoder\n",
      "  File \"C:\\Users\\migue\\Anaconda3\\lib\\json\\__init__.py\", line 238, in dumps\n",
      "    **kw).encode(obj)\n",
      "  File \"C:\\Users\\migue\\Anaconda3\\lib\\site-packages\\plotly\\utils.py\", line 168, in encode\n",
      "    encoded_o = super(PlotlyJSONEncoder, self).encode(o)\n",
      "  File \"C:\\Users\\migue\\Anaconda3\\lib\\json\\encoder.py\", line 199, in encode\n",
      "    chunks = self.iterencode(o, _one_shot=True)\n",
      "  File \"C:\\Users\\migue\\Anaconda3\\lib\\json\\encoder.py\", line 257, in iterencode\n",
      "    return _iterencode(o, 0)\n",
      "  File \"C:\\Users\\migue\\Anaconda3\\lib\\site-packages\\plotly\\utils.py\", line 236, in default\n",
      "    return _json.JSONEncoder.default(self, obj)\n",
      "  File \"C:\\Users\\migue\\Anaconda3\\lib\\json\\encoder.py\", line 180, in default\n",
      "    o.__class__.__name__)\n",
      "TypeError: Object of type 'DataFrame' is not JSON serializable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\migue\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1982, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"C:\\Users\\migue\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1614, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"C:\\Users\\migue\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1517, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"C:\\Users\\migue\\Anaconda3\\lib\\site-packages\\flask\\_compat.py\", line 33, in reraise\n",
      "    raise value\n",
      "  File \"C:\\Users\\migue\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1612, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"C:\\Users\\migue\\Anaconda3\\lib\\site-packages\\flask\\app.py\", line 1598, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"C:\\Users\\migue\\Anaconda3\\lib\\site-packages\\dash\\dash.py\", line 966, in dispatch\n",
      "    return self.callback_map[target_id]['callback'](*args)\n",
      "  File \"C:\\Users\\migue\\Anaconda3\\lib\\site-packages\\dash\\dash.py\", line 921, in add_context\n",
      "    self._validate_callback_output(output_value, output)\n",
      "  File \"C:\\Users\\migue\\Anaconda3\\lib\\site-packages\\dash\\dash.py\", line 867, in _validate_callback_output\n",
      "    _validate_value(output_value)\n",
      "  File \"C:\\Users\\migue\\Anaconda3\\lib\\site-packages\\dash\\dash.py\", line 860, in _validate_value\n",
      "    toplevel=True\n",
      "  File \"C:\\Users\\migue\\Anaconda3\\lib\\site-packages\\dash\\dash.py\", line 802, in _raise_invalid\n",
      "    bad_val=bad_val).replace('    ', ''))\n",
      "dash.exceptions.InvalidCallbackReturnValue: \n",
      "The callback for property `children` of component `output-container-button`\n",
      "returned a value having type `DataFrame`\n",
      "which is not JSON serializable.\n",
      "\n",
      "The value in question is either the only value returned,\n",
      "or is in the top level of the returned list,\n",
      "and has string representation\n",
      "`   Article Title  \\\n",
      "0  Security reasons cited to prevent reburial of ...   \n",
      "1  ‘Open Arms’ rescue ship arrives in Spain with ...   \n",
      "2  The curious Canary Islands rock formation that...   \n",
      "3  Popular Party takes step closer to power in An...   \n",
      "\n",
      "Article Link Prediction  \n",
      "0  https://elpais.com/elpais/2018/12/28/inenglish...   politics  \n",
      "1  https://elpais.com/elpais/2018/12/28/inenglish...   politics  \n",
      "2  https://elpais.com/elpais/2018/12/28/inenglish...  sport  \n",
      "3  https://elpais.com/elpais/2018/12/27/inenglish...   politics  `\n",
      "\n",
      "In general, Dash properties can only be\n",
      "dash components, strings, dictionaries, numbers, None,\n",
      "or lists of those.\n",
      "\n",
      "127.0.0.1 - - [29/Dec/2018 19:50:08] \"POST /_dash-update-component HTTP/1.1\" 500 -\n"
     ]
    }
   ],
   "source": [
    "app.run_server(debug=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
